{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebNLG Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pandas\n",
    "!pip3 install bert-score\n",
    "\n",
    "!pip3 install nltk\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# !pip3 install -r ../Evaluation/rouge/requirements.txt\n",
    "!pip3 install rouge-score\n",
    "!pip3 install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from Import import create_dataframe\n",
    "from Export import *\n",
    "from Evaluation import *\n",
    "from Models import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Separate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"llama-7b_zero-shot-chat_2023-06-17_19-0\"\n",
    "webnlg_file_path = \"./webnlg-dataset/release_v3.0/en/json/test/v3.0_test_set_with_refs.json\"\n",
    "data_set_size = 1779\n",
    "model = ModelType.LLAMA\n",
    "prompt_type = \"zero-shot-chat\"\n",
    "\n",
    "predictions_df = pd.read_csv(f\"Results/csv/Predictions/Test/{file_name}.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is only needed if the lexicalisations are not already stored in predictions_df. This might be the case if you used the test set without references.  \n",
    "Otherwise, skip the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_size = data_set_size\n",
    "# webnlg_df = create_dataframe(webnlg_file_path, data_set_size, predictions_size, False)\n",
    "# predictions_df[\"lexicalisations\"] = webnlg_df[\"lexicalisations\"]\n",
    "# predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evaluation_df(predictions_df: pd.DataFrame, model: ModelType, clean: bool) -> pd.DataFrame:\n",
    "    ''' Adds the results of the given predictions_df to a new evaluation_df '''\n",
    "    evaluation_df = pd.DataFrame()\n",
    "    # Add id to the evaluation dataframe\n",
    "    evaluation_df['id'] = predictions_df['id']\n",
    "\n",
    "    # Add category to the evaluation dataframe\n",
    "    evaluation_df['category'] = predictions_df['category']\n",
    "\n",
    "    # Add triple set size to the evaluation dataframe\n",
    "    evaluation_df['tripleset_size'] = predictions_df['triplesetsize']\n",
    "\n",
    "    # Add modified tripleset to the evaluation dataframe\n",
    "    evaluation_df['tripleset'] = predictions_df['modifiedtripleset']\n",
    "\n",
    "    # Add lexicalisations to the evaluation dataframe\n",
    "    evaluation_df['lexicalisations'] = [get_lexicalisation_of_references(x, False) for x in predictions_df[\"lexicalisations\"]]\n",
    "\n",
    "    # Add predictions to the evaluation dataframe\n",
    "    if clean:\n",
    "      evaluation_df[f'prediction_{model.value}'] = [clean_response(pred) for pred in predictions_df[f'prediction_{model.value}']]\n",
    "    else: \n",
    "      evaluation_df[f'prediction_{model.value}'] = predictions_df[f'prediction_{model.value}']\n",
    "\n",
    "    # Add BLEU scores to the evaluation dataframe\n",
    "    all_bleu_scores = [get_bleu_score_for_prediction(x[0], x[1], False) for x in zip(evaluation_df[f'prediction_{model}'], evaluation_df['lexicalisations'])]\n",
    "    evaluation_df[f'bleu_nltk_{model}'] = all_bleu_scores\n",
    "\n",
    "    # Add METEOR scores to the evaluation dataframe\n",
    "    all_meteor_scores = [get_meteor_score_for_prediction(x[0], x[1], False) for x in zip(evaluation_df[f'prediction_{model}'], evaluation_df['lexicalisations'])]\n",
    "    evaluation_df[f'meteor_{model}'] = all_meteor_scores\n",
    "\n",
    "    # Add Bert scores to the evaluation dataframe\n",
    "    # all_bert_scores = [get_all_bert_scores_for_prediction(x[0], x[1], False) for x in zip(evaluation_df[f'prediction_{model}'], evaluation_df['lexicalisations'])]\n",
    "    # evaluation_df[f'bert_precision_{model}'] = [x[0] for x in all_bert_scores]\n",
    "    # evaluation_df[f'bert_recall_{model}'] = [x[1] for x in all_bert_scores]\n",
    "    # evaluation_df[f'bert_f1_{model}'] = [x[2] for x in all_bert_scores]\n",
    "\n",
    "    # Add execution time to the evaluation dataframe\n",
    "    evaluation_df[f'execution_time_{model}'] =predictions_df[f'execution_time_{model}']\n",
    "    \n",
    "    return evaluation_df\n",
    "\n",
    "evaluation_df = create_evaluation_df(predictions_df, model, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_dataframe_to_csv(evaluation_df, \"Results/csv/Evaluations/Test\", f\"{file_name}_cleaned\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Overall Predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for Official Evaluation Script"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform Hypothesis to .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"copy-baseline_2023-06-18_14-47\"\n",
    "prediction_column = \"prediction_copy-baseline\"\n",
    "predictions_df = pd.read_csv(f\"Results/csv/Predictions/Test/{file_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clean_response(\"Output text: Mermaid (Train song) was followed by Imagine (John Lennon song) which was produced by Espionage (production team) and written by Pat Monahan. USER: Input triples: [{'object': 'Mike_Pence', 'property': '\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df[prediction_column] = [clean_response(x) for x in predictions_df[prediction_column]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_predictions_to_file(predictions_df, \"Results/txt/Predictions/Test\", f\"{file_name}\", prediction_column)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute the overall evaluation you can use the [official script](https://github.com/WebNLG/GenerationEval)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webnlg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
