{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "244727b5-463a-447e-8965-e30475a9e849",
   "metadata": {},
   "source": [
    "# WebNLG Data-To-Text Predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4725266-5256-456d-90cb-4d46ba7cf0cb",
   "metadata": {},
   "source": [
    "#### Download the WebNLG dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b984f6e2-1884-4df4-b282-ea4dc5055a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://gitlab.com/shimorina/webnlg-dataset.git"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6fec587f",
   "metadata": {},
   "source": [
    "### Setup WebNLG"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49172f26-4f64-45f8-b7cf-016915c3d129",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56590aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pandas\n",
    "!pip3 install openai\n",
    "!pip3 install python-dotenv             \n",
    "!pip3 install typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f72bf6-29d2-4969-b7dd-2f447e9e554a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from Import import create_webnlg_df, create_dataframe\n",
    "from Requests import *\n",
    "from Export import *\n",
    "from Prompts import *\n",
    "from Models import *\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8057104-cb44-4f8e-88de-8587a2555354",
   "metadata": {},
   "source": [
    "#### Create Dataframe from WebNLG dataset\n",
    "\n",
    "Add first 10 values to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9cb387-068e-4942-b8e6-f0e2bbc81d94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = \"./webnlg-dataset/release_v3.0/en/json/dev/v3.0_dev_set.json\"\n",
    "data_set_size = 1667\n",
    "sample_size = 20\n",
    "\n",
    "webnlg_df = create_dataframe(file_path, data_set_size, sample_size, random_sample=True)\n",
    "webnlg_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0817ca3-b257-4728-a6ed-5fd074c1067a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Call LLM APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6101827",
   "metadata": {},
   "outputs": [],
   "source": [
    "model: ModelType = ModelType.GPT3\n",
    "\n",
    "print(model.value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81340c32",
   "metadata": {},
   "source": [
    "#### OpenAI Chat models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834ceb7b-4d9f-4788-8182-cd3867d6e43d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = ModelType.GPT3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca7eb88-9c38-4d15-8c1f-1970c16ee3ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = send_to_openai_chat([{\"role\": \"user\", \"content\": 'Hi, how are you doing'}], model.value)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554943f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_triple_to_text_openai_chat(model: str, input_tripleset, print_prompt_template: bool = False) -> Tuple[str, float]:\n",
    "    prompt_template = get_few_shot_chat_prompt(input_tripleset, True)\n",
    "    \n",
    "    if print_prompt_template:\n",
    "        print(f\"Prompt template: {prompt_template}\")\n",
    "    \n",
    "    return send_to_openai_chat(prompt_template, model)\n",
    "\n",
    "input = webnlg_df[\"modifiedtripleset\"][2]\n",
    "result = convert_triple_to_text_openai_chat(model.value, input, True)\n",
    "print(f\"\"\"\n",
    "Output: {result[0]}\n",
    "Execution time: {result[1]}\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12f3d374",
   "metadata": {},
   "source": [
    "Fill the dataframe with OpenAi gpt3 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e072e10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_gpt3_predictions_to_df(model: str, dataframe: pd.DataFrame, input_column: str) -> pd.DataFrame:\n",
    "    # Get the predictions for all entries of the input_column in the dataframe\n",
    "    response = [convert_triple_to_text_openai_chat(model, x) for x in dataframe[input_column]]\n",
    "\n",
    "    predictions = [x[0] for x in response]\n",
    "    execution_time = [x[1] for x in response]\n",
    "\n",
    "    dataframe[f\"prediction_{model}\"] = predictions\n",
    "    dataframe[f\"execution_time_{model}\"] = execution_time\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "webnlg_df = add_gpt3_predictions_to_df(model.value, webnlg_df, \"modifiedtripleset\")\n",
    "webnlg_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b7654c1",
   "metadata": {},
   "source": [
    "#### OpenAI Completion Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7fbdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelType.DAVINCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531bfe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_triple_to_text_openai_completion(model: ModelType, input_tripleset, print_prompt_template: bool = False) -> Tuple[str, float]:\n",
    "    prompt_template = get_zero_shot_completion_prompt(input_tripleset)\n",
    "\n",
    "    if print_prompt_template:\n",
    "        print(f\"Prompt template: {prompt_template}\")\n",
    "    \n",
    "    return send_to_openai_completion(prompt_template, model.value)\n",
    "\n",
    "input = webnlg_df[\"modifiedtripleset\"][2]\n",
    "result = convert_triple_to_text_openai_completion(model, input, True)\n",
    "print(f\"\"\"\n",
    "Output: {result[0]}\n",
    "Execution time: {result[1]}s\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30b1d72c",
   "metadata": {},
   "source": [
    "Fill the dataframe with OpenAi text-davinci-003 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd20ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_davinci_predictions_to_df(model: ModelType, dataframe: pd.DataFrame, input_column: str) -> pd.DataFrame:\n",
    "    # Get the predictions for all entries of the input_column in the dataframe\n",
    "    response = [convert_triple_to_text_openai_completion(model, x) for x in dataframe[input_column]]\n",
    "\n",
    "    predictions = [x[0] for x in response]\n",
    "    execution_time = [x[1] for x in response]\n",
    "\n",
    "    dataframe[f\"prediction_{model.value}\"] = predictions\n",
    "    dataframe[f\"execution_time_{model.value}\"] = execution_time\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "webnlg_train_df = add_davinci_predictions_to_df(model, webnlg_df, \"modifiedtripleset\")\n",
    "webnlg_train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02dbc2f2",
   "metadata": {},
   "source": [
    "#### Local Server (Vicuna / LLaMA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47949c55",
   "metadata": {},
   "source": [
    "The models that run on the local server with FastChat (i.e. LLaMA, Vicuna), do not need to send a system message to the server in the api call. For these models the system message is already added as part of FastChat conversation template (it can be changed in the file fastchat/conversation.py).  \n",
    "To verify that the correct system message is used, you can make the following request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dad0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl --location --request POST 'http://0.0.0.0:21002/worker_get_conv_template'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e241c4dc-2ae9-4529-9118-a354bab3a99c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"VICUNA\" or \"LLAMA\"\n",
    "model = ModelType.LLAMALORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac785e9-6202-4838-a553-f2e97d005702",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = send_to_local_server_chat([{\"role\": \"user\", \"content\": 'What have I asked you before?'}], model.value)\n",
    "print(f\"Answer: {result[0]}\\nExecution time: {result[1]}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4fdf19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_triple_to_text_local_server_chat(model: ModelType, input_tripleset, print_prompt_template: bool = False) -> Tuple[str, float]:\n",
    "    prompt_template = get_few_shot_chat_prompt(input_tripleset)\n",
    "    \n",
    "    if print_prompt_template:\n",
    "        print(f\"Prompt template: {prompt_template}\")\n",
    "\n",
    "    return send_to_local_server_chat(prompt_template, model.value)\n",
    "    \n",
    "\n",
    "input = webnlg_df[\"modifiedtripleset\"][0]\n",
    "result = convert_triple_to_text_local_server_chat(model, input, True)\n",
    "print(f\"\"\"\n",
    "Output: {result[0]}\n",
    "Execution time: {result[1]}\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81c6e519",
   "metadata": {},
   "source": [
    "You can also use the completion endpoint instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efadccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_triple_to_text_local_server_completion(model: ModelType, input_tripleset, print_prompt_template: bool = False) -> Tuple[str, float]:\n",
    "    prompt_template = get_few_shot_completion_prompt(input_tripleset)\n",
    "    \n",
    "    if print_prompt_template:\n",
    "        print(f\"Prompt template: {prompt_template}\")\n",
    "\n",
    "    return send_to_local_server_completion(prompt_template, model.value)\n",
    "    \n",
    "\n",
    "input = webnlg_train_df[\"modifiedtripleset\"][2]\n",
    "result = convert_triple_to_text_local_server_completion(model, input, True)\n",
    "print(f\"\"\"\n",
    "Output: {result[0]}\n",
    "Execution time: {result[1]}s\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a438a59f",
   "metadata": {},
   "source": [
    "Fill the dataframe with Vicuna predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8be32d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_local_model_predictions_to_df(model: ModelType, dataframe: pd.DataFrame, input_column: str) -> pd.DataFrame:\n",
    "    # Get the predictions for all entries of the input_column in the dataframe\n",
    "    response = [convert_triple_to_text_local_server_chat(model, x, True) for x in dataframe[input_column]]\n",
    "    predictions = [x[0] for x in response]\n",
    "    execution_time = [x[1] for x in response]\n",
    "\n",
    "    dataframe[f\"prediction_{model}\"] = predictions\n",
    "    dataframe[f\"execution_time_{model}\"] = execution_time\n",
    "\n",
    "    return dataframe    \n",
    "\n",
    "webnlg_df = add_local_model_predictions_to_df(model, webnlg_df, \"modifiedtripleset\")\n",
    "webnlg_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "accd7336",
   "metadata": {},
   "source": [
    "#### Finetuned Model (LLaMA-Lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd26966",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelType.LLAMALORA\n",
    "\n",
    "print(model.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d440d14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = send_to_local_gradio_server(get_finetune_instruction(), get_fintune_input(webnlg_train_df[\"modifiedtripleset\"][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdd5e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = json.loads(result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4edc2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Answer: {result['data']}\\nExecution time: {result['duration']}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0c1466",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = webnlg_train_df[\"modifiedtripleset\"][2]\n",
    "result = convert_triple_to_text_gradio_server(lambda: get_finetune_instruction(), get_fintune_input(input))\n",
    "print(f\"\"\"\n",
    "Input: {input}\n",
    "Output: {result[0]}\n",
    "Execution time: {result[1]}s\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4303e59b",
   "metadata": {},
   "source": [
    "Fill the dataframe with predictions of local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7a00ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_finetuned_model_predictions_to_df(model: ModelType, dataframe: pd.DataFrame, input_column: str) -> pd.DataFrame:\n",
    "    # Get the predictions for all entries of the input_column in the dataframe\n",
    "    response = [convert_triple_to_text_gradio_server(lambda: get_finetune_instruction(), get_fintune_input(input)) for input in dataframe[input_column]]\n",
    "    predictions = [x[0] for x in response]\n",
    "    execution_time = [x[1] for x in response]\n",
    "\n",
    "    dataframe[f\"prediction_{model.value}\"] = predictions\n",
    "    dataframe[f\"execution_time_{model.value}\"] = execution_time\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "webnlg_train_df = add_finetuned_model_predictions_to_df(model, webnlg_train_df, \"modifiedtripleset\")\n",
    "webnlg_train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7bb57994",
   "metadata": {},
   "source": [
    "#### Copy-Baseline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "853e9585",
   "metadata": {},
   "source": [
    "As a baseline, we simply copy \"object\", \"property\" and \"subject\" to the output. We separate each triple with a \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88928ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelType.BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b16078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_copy_baseline_string_from_triples(triples: list) -> str:\n",
    "    result = \"\"\n",
    "\n",
    "    for triple in triples: \n",
    "        result += f\"{triple['object']} {triple['property']} {triple['subject']}. \"\n",
    "    \n",
    "    return result\n",
    "\n",
    "create_copy_baseline_string_from_triples(webnlg_train_df[\"modifiedtripleset\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a43ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_copy_baseline_to_df(model: ModelType, dataframe: pd.DataFrame, input_column: str) -> pd.DataFrame:\n",
    "    dataframe[f\"prediction_{model.value}\"] = [create_copy_baseline_string_from_triples(x) for x in dataframe[input_column]]\n",
    "    dataframe[f\"execution_time_{model.value}\"] = 0.0\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "webnlg_train_df = add_copy_baseline_to_df(model, webnlg_train_df, \"modifiedtripleset\")\n",
    "webnlg_train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "503ef58a",
   "metadata": {},
   "source": [
    "### Export to Excel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56c009fc",
   "metadata": {},
   "source": [
    "#### Export webnlg_train_df to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cc6e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_dataframe_to_csv(webnlg_df, \"Results/csv/Predictions/Dev\", f\"{model.value}_few-shot-chat\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f950e7a",
   "metadata": {},
   "source": [
    "## Automatic Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613ced5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_dataframe_and_export(dataframe: pd.DataFrame, index: int, prediction: str, execution_time: float, model: ModelType, prompt_type: str, output_path: str):\n",
    "    # add prediction and execution time to the dataframe columns at index\n",
    "    dataframe.loc[index, f\"prediction_{model.value}\"] = prediction\n",
    "    dataframe.loc[index, f\"execution_time_{model.value}\"] = execution_time    \n",
    "\n",
    "    # append the dataframe row at index to the csv file\n",
    "    header = index == 0\n",
    "    dataframe.iloc[[index]].to_csv(f\"{output_path}/{model.value}_{prompt_type}.csv\", mode='a', header=header, index=False)\n",
    "    print(f\"Exported prediction for example with id: {dataframe.loc[index, 'id']}\")\n",
    "\n",
    "# append_to_dataframe_and_export(webnlg_df, 0, \"This is the test prediction 0\", 0.1, ModelType.LLAMALORA, \"test-prompt\", \"Results/csv/Predictions/Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827b6324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_llama_lora(dataframe: pd.DataFrame, input_column: str, sample_size: int, start_index: int, output_path: str) -> pd.DataFrame:\n",
    "    inputs = dataframe[input_column]\n",
    "\n",
    "    for index, input in enumerate(inputs):\n",
    "        if (index < start_index):\n",
    "            continue\n",
    "\n",
    "        if (index == (start_index + sample_size)):\n",
    "            break\n",
    "\n",
    "        response = convert_triple_to_text_gradio_server(lambda: get_finetune_instruction(), get_fintune_input(input))\n",
    "\n",
    "        prediction = response[0]\n",
    "        execution_time = response[1]\n",
    "\n",
    "        append_to_dataframe_and_export(dataframe, index, prediction, execution_time, ModelType.LLAMALORA, \"finetuned\", output_path)\n",
    "\n",
    "    return dataframe\n",
    "        \n",
    "\n",
    "def create_predictions_for_dataframe(model: ModelType, dataset_path: str, dataset_size: int, sample_size: int, prompt_generator: Callable[[str], str], prompt_type: str, output_path: str, start_id: int = 1) -> pd.DataFrame:\n",
    "    '''start_id is the id of the first example in the dataset that should be used for prediction (assuming the dataset is sorted by id ascending)'''\n",
    "    start_index = start_id - 1\n",
    "    max_tokens = 128\n",
    "    input_column = \"modifiedtripleset\"\n",
    "    prompt_templates: List[List[dict]]\n",
    "    dataframe = create_webnlg_df(dataset_path, dataset_size)\n",
    "\n",
    "    if (model is ModelType.LLAMALORA):\n",
    "        return handle_llama_lora(dataframe, input_column, sample_size, start_index, output_path)\n",
    "\n",
    "    prompt_templates = [prompt_generator(x) for x in dataframe[input_column]]\n",
    "    print(prompt_templates[start_index])\n",
    "\n",
    "    for index, prompt_template in enumerate(prompt_templates):\n",
    "        start_time = time.time()\n",
    "        if (index < start_index):\n",
    "            continue\n",
    "\n",
    "        if (index == (start_index + sample_size)):\n",
    "            break\n",
    "\n",
    "        if (model is ModelType.LLAMA or model is ModelType.VICUNA or model is ModelType.LORA):\n",
    "            # if you want to test the completion endpoints of LLaMA or Vicuna you can change the line below to:\n",
    "            # response = send_to_local_server_completion(prompt_template, model.value)\n",
    "            response = send_to_local_server_chat(prompt_template, model.value, max_tokens=max_tokens)\n",
    "        elif (model is ModelType.GPT3):\n",
    "            response = send_to_openai_chat(prompt_template, model.value, max_tokens=max_tokens)\n",
    "        elif (model is ModelType.DAVINCI):\n",
    "            response = send_to_openai_completion(prompt_template, model.value, max_tokens=max_tokens)\n",
    "\n",
    "        prediction = response[0]\n",
    "        execution_time = response[1]\n",
    "\n",
    "        append_to_dataframe_and_export(dataframe, index, prediction, execution_time, model, prompt_type, output_path)\n",
    "        print(time.time() - start_time)\n",
    "    return dataframe\n",
    "    \n",
    "    \n",
    "predictions_df = create_predictions_for_dataframe(ModelType.LORA, \"./webnlg-dataset/release_v3.0/en/json/test/v3.0_test_set.json\", 1779, 1779, lambda x: get_few_shot_chat_prompt(x, False), \"few-shot-chat\", \"Results/csv/Predictions/Test\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0057f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0933b152",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
