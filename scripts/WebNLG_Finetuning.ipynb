{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebNLG \n",
    "\n",
    "* We use https://github.com/git-cloner/llama-lora-fine-tuning/tree/main to finetune LLaMA-7B using the chat format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Clone the Repository and remove fschat from requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/git-cloner/llama-lora-fine-tuning.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Clone the FastChat library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/lm-sys/FastChat.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Adapt the system message of the vicuna_v1.1 conversation template (this is used for finetuning) in fastchat/conversation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Build the FastChat library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Install all dependencies of the llama-lora-fine-tuning repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Run the fine-tuning command\n",
    "* You might need to adapt some parameters based on your hardware i.e. GPU\n",
    "* model_name_or_path is the path to the LLaMA weights\n",
    "* data_path is the path to the fine-tuning dataset\n",
    "* output_dir is the directory in which the adapter will be placed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!deepspeed fastchat/train/train_lora.py \\\n",
    "    --deepspeed ./deepspeed-config.json \\\n",
    "    --lora_r 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0.05 \\\n",
    "    --model_name_or_path ~/Development/LLM/LLAMA/7B/Transformer/ \\\n",
    "    --data_path ~/Development/LLM/FastChat/data/webnlg_finetune_dataset_chat.json \\\n",
    "    --fp16 True \\\n",
    "    --output_dir ./output/lora-adapter/webnlg-adapter \\\n",
    "    --num_train_epochs 5 \\\n",
    "    --per_device_train_batch_size 8 \\\n",
    "    --per_device_eval_batch_size 8 \\\n",
    "    --gradient_accumulation_steps 1\\\n",
    "    --evaluation_strategy \"no\" \\\n",
    "    --save_strategy \"steps\" \\\n",
    "    --save_steps 1200 \\\n",
    "    --save_total_limit 1 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --weight_decay 0. \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --lr_scheduler_type \"cosine\" \\\n",
    "    --logging_steps 1 \\\n",
    "    --model_max_length 512 \\\n",
    "    --gradient_checkpointing True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Merge the LoRA adapter into the LLaMA weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m fastchat.model.apply_lora --base /home/ubuntu/Development/LLM/LLAMA/7B/Transformer --target /home/ubuntu/Development/LLM/Lora/llama-lora-fine-tuning/output/lora-model/webnlg-model --lora /home/ubuntu/Development/LLM/Lora/llama-lora-fine-tuning/output/lora-adapter/webnlg-adapter"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
