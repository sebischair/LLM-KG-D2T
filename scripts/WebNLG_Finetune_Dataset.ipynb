{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebNLG Finetune Dataset\n",
    "\n",
    "The required structure of the dataset is as follows:\n",
    "```\n",
    "[{\n",
    "        \"instruction\": \"\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"\"\n",
    "}]\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Import import create_webnlg_df\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "import json\n",
    "from Prompts import get_finetune_instruction\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dataframe from Webnlg Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./webnlg-dataset/release_v3.0/en/json/train/v3.0_train_set.json\"\n",
    "train_set_size = 13211\n",
    "\n",
    "webnlg_train_df = create_webnlg_df(file_path, train_set_size)\n",
    "webnlg_train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Finetune Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lixicalisation(lexicalisations: List[Dict[str, str]]) -> str:\n",
    "    \"\"\"Returns a random lexicalisation from the list of lexicalisations with comment good\"\"\"\n",
    "    good_lexes = []\n",
    "    for lexicalisation in lexicalisations:\n",
    "        if lexicalisation[\"comment\"] == \"good\":\n",
    "            good_lexes.append(lexicalisation[\"lex\"])\n",
    "\n",
    "    if good_lexes == []:\n",
    "        return None\n",
    "    \n",
    "    return random.choice(good_lexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_finetune_dataset(webnlg_train_df: pd.DataFrame, train_set_size: int, conversations: int):\n",
    "    finetune_dataset: List(dict) = []\n",
    "\n",
    "    for i in range(conversations):\n",
    "        conversation_turns = random.randint(1, 4)\n",
    "        conversations = []\n",
    "\n",
    "        for j in range(conversation_turns):\n",
    "            # select a random index between 0 and train_set_size\n",
    "            df_index = random.randint(0, train_set_size - 1)\n",
    "            lex = get_lixicalisation(webnlg_train_df[\"lexicalisations\"][df_index])\n",
    "  \n",
    "            if lex is None:\n",
    "              # skip this row\n",
    "              j -= 1\n",
    "              continue\n",
    "            \n",
    "            conversations.append(\n",
    "                {\n",
    "                  \"from\": \"human\",\n",
    "                  \"value\": f\"Input triples: {webnlg_train_df['modifiedtripleset'][df_index]}\"\n",
    "                })\n",
    "            conversations.append(\n",
    "             {\n",
    "                \"from\": \"gpt\",\n",
    "                \"value\": f\"Output text: {lex}\"\n",
    "              }\n",
    "            )\n",
    "\n",
    "        finetune_dataset.append({\n",
    "            \"id\": f\"identity_{i}\",\n",
    "            \"conversations\": conversations\n",
    "        })\n",
    "        \n",
    "    return finetune_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using train_set_size conversations leads on average to every sample beeing included two and a half times (using randomly choosen lexicalisations)\n",
    "conversations = train_set_size * 2\n",
    "finetune_dataset = create_finetune_dataset(webnlg_train_df, train_set_size, conversations)\n",
    "print(finetune_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export finetune dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_dataset_to_json_file(dataset: List[dict], file_path: str):\n",
    "    with open(file_path, \"w\") as file:\n",
    "        json.dump(dataset, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_dataset_to_json_file(finetune_dataset, \"./webnlg_finetune_dataset_chat.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webnlg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
